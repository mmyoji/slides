# Introduction to ML with Python Ch2

2017/11/09 @mmyoji

---

## 目次

* 教師あり学習アルゴリズム
  * ナイーブベイズクラス分類器
  * 決定木
  * ランダムフォレスト
  * 勾配ブースティングマシン
  * カーネル法を用いたSVM

---

### ナイーブベイズクラス分類器

* 線形モデルより訓練が高速
* 汎化性能が僅かに劣ることが多い
* ? クラスに対する統計値を個々の特徴量ごとに集めて、パラメータを学習するから速い

---

* GaussianNB
  * 任意の連続値データに適用可. 高次元なデータに用いられる
* BernoulliNB
  * 2値データを仮定
  * 個々のクラスに対して、特徴量ごとに非ゼロである場合をカウントする
* MultinomialNB
  * カウントデータを仮定
  * 一般的には BernoulliNB よりは性能がいい
    * 多数の非ゼロ特徴量がある大きなドキュメントなどに有効
---

### カウントデータ

* 個々の特徴量が何らかの整数カウントを表現しているデータ
* e.g.) 文中に出てくる単語の出現数

---

### ナイーブベイズクラス分類器

#### pros

* Linear Model ですら時間がかかるような巨大なデータセットに対するベースラインモデルとして適当

#### cons

* クラス分類でしか使えない

---

## 決定木

* Yes/No で答えられる質問で構成された階層的な木構造を学習
* e.g.) 4種類(classes)の動物（熊、鷹、ペンギン、イルカ）を区別
* 連続値の場合は Yes/No ではなく、「特徴量 i は値 a よりも大きいか？」となる
* 葉がすべて pure になるまで分割し続けると過剰適合してしまう
  * 事前/事後枝刈りで対応

---

## 決定木

* 事前枝刈り pre-pruning:
  * 木の生成を早めに止める
* (事後)枝刈り (post-)pruning:
  * 一度木を構築してから情報の少ないノードを削除

---

## 決定木

#### pros

* 結果のモデルが容易に可視化可能
* 特徴量の正規化や標準化が必要ない
  * 個々の特徴量は独立に処理され、データの分割はスケールに依存しないので

#### cons

* 事前枝刈りしたとしても、過剰適合しやすい
  * -> 汎化性能が低い傾向

---

### 決定木のアンサンブル法 Ensembles

* ほとんどのケースでは決定木単体では使えない
* 複数の機械学習を組み合わせる手法
* ensemble: 仏語で「一緒に」という意味

---

## ランダムフォレスト

* 異なった方向に過剰適合した複数の決定木を作り、それらの平均を取ることで過剰適合を防ぐ
* デフォルトの params でも高い性能が出る

---

## ランダムフォレスト

#### pros

* 並行処理ができるのでコア数が多い場合は速く処理できる
* 性能が高い

#### cons

* テキストデータなどの、非常に高次元で疎なデータに対してはうまく機能しない傾向
  * その場合は Linear Model の方がいい
* メモリ消費量多い
* 訓練、予測ともに LM より遅い

---

### 勾配ブースティングマシン

* 一つ前の決定木の誤りを次の決定木が修正するようにして決定木を生成していく
* 深さ1~5程度の非常に浅い決定木が用いられる
  * メモリ消費が少なくなり、予測も速くなる
* params の設定さえちゃんとすればかなり性能はいい

---

### 勾配ブースティングマシン

* 基本的には頑健な RF から先に試す
  * 頑健な: ここでは params の影響を受けづらいという意味
  * 予測時間が非常に重要だったり、1%まで性能を絞り出したい場合, GB を使う
* [xgboost pkg](http://xgboost.readthedocs.io/en/latest/python/python_intro.html) の方が高速でつかいやすい

---

### 勾配ブースティングマシン

#### pros

* 教師あり学習の中で最も強力

#### cons

* params の細かいチューニングが必要
* 訓練時間が長い
* 高次元で疎なデータに対してはうまく機能しない傾向

---

## カーネル法を用いたSVM

* より複雑なモデルを可能にするため、線形SVMを拡張したもの
* 背後にある数学はかなり難しい #ﾄﾉｺﾄ
* 非線形の特徴量をデータ表現に加えることで、線形モデルを強力にする

---

## カーネル法を用いたSVM

そもそも SVM とは

* 訓練の過程で、SVMは個々のデータポイントが2つのクラスの決定境界を表現するのにどの程度重要かを学習
* 多くの場合、2つのクラスの境界に位置するごく一部の訓練データポイントだけが決定境界を決定する
  * -> これらのデータポイントを support vector と呼ぶ

---

## カーネル法を用いたSVM

Kernel Trick

* 沢山の特徴量を加えた場合にうまいこと計算するテクニック
* 高次元空間へのマッピング方法(数学的な背景は実践上は必要ないとのこと)

---

## カーネル法を用いたSVM

Kernel Trick

1. 多項式カーネル polynomial kernel
  * 元の特徴量の特定の次数までのすべての多項式を計算する
2. ガウシアンカーネル, 放射基底関数(RBF)
  * すべての次数のすべての多項式を考えるが、次数が高くなるにつれてその特徴量の重要性を低くする

---

## カーネル法を用いたSVM

#### pros

* データに僅かな特徴量しかない場合でも複雑な決定境界を生成することが可能

#### cons

* メモリ使用量が多い
  * 100,000サンプルくらいになるとメモリ使用量的に厳しい
* 適切なデータの前処理, params 調整が必要
* 検証が難しい

---

おわり
