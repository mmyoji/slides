# Intro to ML with Python 4th

2017/11/22 @mmyoji

---

## 目次

4章 データの表現と特徴量エンジニアリング

* 4.1 カテゴリ変数
* 4.2 ビニング、離散化、線形モデル、決定木
* 4.3 交互作用と多項式

---

## 特徴量の種類

* 連続値特徴量: データは2次元の浮動小数点配列
  * e.g. ピクセルの明るさ、花の大きさ
* 離散値特徴量: 数値ではない
  * カテゴリ特徴量など
  * e.g. 製品のブランドや色

---

## 特徴量エンジニアリング

* 特定のアプリケーションに対して、最良のデータ表現を模索する
* データを正しく表現する

---

## カテゴリ変数

* カテゴリ特徴量と言ってもよい?
* one-hot-encoding という手法で数値として表現
* P.207 表4-2
* pandas の `get_dummies` 関数が便利

---

## カテゴリ変数

つづき

* データが文字列で与えられている場合、表記揺れに気をつける
  * e.g. 「男性」「男」「♂」などは同じものとして扱いたい
* 数値データで入っているが、実態としては離散値特徴量の場合も気をつける
  * e.g. 好きな色: 赤 0, 青 1, 黄 2

---

## ビニング、離散化

* 「常にこれがベスト」と呼べるデータの形式はない
* アルゴリズムに合わせて最適な表現方法を使う
* 線形モデルに対しては **ビニング** (**離散化**) を使う

---

## ビニング、離散化

* ビニング binning / 離散化 discretization
  * 特徴量の入力レンジを特定の個数に分割する
  * この離散値特徴量を one-hot-encoding して用いる
* それぞれのレンジを bin と呼ぶ
* 決定木は最初から柔軟なのでこれは必要ない
* どうしても線形モデルを使う必要がある場合はこれを用いると表現力UP

---

## 交互作用と多項式

* 特に線形モデルに対して、 交互作用特徴量と多項式特徴量を加えると表現力が上がる

---

## 交互作用と多項式

交互作用特徴量 interaction feature

* `np.hstack([X_binned, X * X_binned])`
* ビニングしたデータと、元のデータとビニングしたデータの積を特徴量として加える

---

## 交互作用と多項式

多項式特徴量 polynomial feature

* 元の特徴量の多項式を用いる
* `X, X^2, X^3, X^4, ...`
* 必ずしも性能をよくするわけではないので、アルゴリズムによって使い分ける

---

おわり
