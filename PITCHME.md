## Intro to ML with Python 7th

2017/12/14 @mmyoji

---

## 範囲

7章7.7まで

---

### 文字列として表現されているデータのタイプ

* カテゴリデータ
* 意味的にはカテゴリに分類できる自由に書かれた文字列
* 構造化された文字列
* テキストデータ

---

### カテゴリデータ

* 固定されたリストから得られるデータ
* e.g. "red", "green", "blue"

---

### 意味的にはカテゴリに分類できる自由に書かれた文字列

* e.g. 「好きな色は？」という質問に対して自由記述形式で答えさせて得られたデータ

---

### 構造化された文字列

* 固定したカテゴリには属していなくても何かしらの構造を持つ値
* e.g. 住所, 人名, 日付

---


### テキストデータ

* 単語で構成される文書
* e.g. tweet, チャットログ, レビュー

---

### テキスト解析における用語

* コーパス: データセット, 文書の集合
* 文書 document: データポイント

---

### Bag of Words によるテキスト表現

* 映画のレビューを見て、それぞれが「肯定的」「否定的」かどうか判断したい
* ただのテキストをそのままMLに適用できないので数値に変換する必要がある
* BoW はそのうちの一手法
* コーパスに現れた単語がテキストに現れる回数だけ数えられる
* もとの文字列での単語の順番は失われる

---

## BoW の3つのステップ

1. トークン分割 Tokenization
  * 単語 Token に個々の文書を分割する
2. ボキャブラリ構築 Vocabulary building
  * コーパスに現れる全単語をボキャブラリとして集め、番号をつける
3. エンコード
  * 個々の文書に対して 2. の単語が現れる回数を数える

---

### min_df で不要なデータを削る

* 意味のない数字や動詞の変化形などが個々の特徴量として表現されてしまう
* n 個以上の文書に現れる単語のみ token として採用する、ということを `min_df` で指定

---

## ストップワードを除く

* 言語毎にどんな文書にも頻出する単語は意味がないので削る

---

### tf-idf を用いたデータのスケール変換

* 特定の文書にだけ頻出する単語に重みをつける
* 逆にコーパス中の多くの文書に頻出する単語は重みを小さくする

---

## n-gram

* BoW 表現はあくまで1単語ごとに見る
* 2 token, 3 token など token の塊で考える
* n-gram の n を大きくすればするほど特徴量が増加するのでやりすぎ注意

---

おわり
